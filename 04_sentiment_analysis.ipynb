{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "- 단어 추출(토크나이저 재구성)\n",
    "- 사전 만들기\n",
    "- 문서 - 단어 - 벡터화(매트릭스 만들기)\n",
    "- 시기 별(1개월 단위로) \"박근혜\", \"대통령\", \"당선인\" 등 박근혜 전 대통령을 지칭하는 키워드와 유사어 추출\n",
    "\n",
    "- 워드클라우드로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expressive-type.csv       polarity.csv              subjectivity-type.csv\n",
      "intensity.csv             readme.txt\n",
      "nested-order.csv          subjectivity-polarity.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ./lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>2017.03.06</td>\n",
       "      <td>[사설] 운명의 일주일, '탄핵' '기각' 이후가 더 중요하다</td>\n",
       "      <td>이르면 이번 주 후반 박근혜 대통령 탄핵 심판 사건에 대한 헌재 결정이 나올 가능성...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>2017.03.07</td>\n",
       "      <td>[사설] 중국 땅인지 착각게 한 롯데 앞 촛불 시위대</td>\n",
       "      <td>지난 4일 밤 광주 롯데백화점 앞 촛불 시위 사진은 눈을 의심케 했다. 수백 명이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>2017.03.07</td>\n",
       "      <td>[사설] 특검, '최순실 국정농단'의 本流 꿰뚫은 수사였나</td>\n",
       "      <td>박영수 특검팀이 6일 수사 결과를 발표했다. 그러나 평가는 엇갈린다. 구속기소된 이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>2017.03.09</td>\n",
       "      <td>[사설] 10일 탄핵 심판 선고, 모두 自重하고 또 自制하자</td>\n",
       "      <td>박근혜 대통령 탄핵 심판 선고 기일이 10일로 확정됐다. 헌재 재판관 8명이 8일 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>2017.03.10</td>\n",
       "      <td>[사설] 오늘 시험대 오르는 대한민국, '역사적 승복'으로 위기 끝내자</td>\n",
       "      <td>헌법재판소가 오늘 11시 박근혜 대통령 탄핵심판 사건을 선고한다. 작년 10월 5일...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                    title  \\\n",
       "1017  2017.03.06       [사설] 운명의 일주일, '탄핵' '기각' 이후가 더 중요하다   \n",
       "1018  2017.03.07            [사설] 중국 땅인지 착각게 한 롯데 앞 촛불 시위대   \n",
       "1019  2017.03.07         [사설] 특검, '최순실 국정농단'의 本流 꿰뚫은 수사였나   \n",
       "1020  2017.03.09        [사설] 10일 탄핵 심판 선고, 모두 自重하고 또 自制하자   \n",
       "1021  2017.03.10  [사설] 오늘 시험대 오르는 대한민국, '역사적 승복'으로 위기 끝내자   \n",
       "\n",
       "                                                   body  \n",
       "1017  이르면 이번 주 후반 박근혜 대통령 탄핵 심판 사건에 대한 헌재 결정이 나올 가능성...  \n",
       "1018  지난 4일 밤 광주 롯데백화점 앞 촛불 시위 사진은 눈을 의심케 했다. 수백 명이 ...  \n",
       "1019  박영수 특검팀이 6일 수사 결과를 발표했다. 그러나 평가는 엇갈린다. 구속기소된 이...  \n",
       "1020  박근혜 대통령 탄핵 심판 선고 기일이 10일로 확정됐다. 헌재 재판관 8명이 8일 ...  \n",
       "1021  헌법재판소가 오늘 11시 박근혜 대통령 탄핵심판 사건을 선고한다. 작년 10월 5일...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_pickle('cs_whole.pkl')\n",
    "data_raw.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# erase \\n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"19일 실시된 대통령선거에서 새누리당 박근혜 후보가 투표자 3072만명 중 51% 남짓 1600만표가량을 얻어 18대 대통령에 당선됐다. 민주통합당 문재인 후보는 48% 남짓 1500만표가량을 얻었다. 1987년 직선제 개헌 이후 득표율 50%를 넘긴 대통령은 이번이 처음이다. 70%를 약간 넘을 것으로 예상했던 투표율은 75.8%까지 올랐다. 대한민국 건국 이후 첫 여성 대통령이 탄생했다. 박 후보의 당선으로 투표율이 높으면 야당이 유리하다는 속설(俗說)도 깨졌다.\\n경쟁자 지지 국민의 박탈감 헤아려야\\n\\n정권 재창출보다 정권 교체를 바라는 여론이 더 높은 가운데 치러진 이번 대선은 박 후보에게 버거운 선거였다. 선거 초반 한동안 유지되던 박근혜 대세론은 무소속 안철수 후보 등장으로 몇 차례 크게 흔들렸다. 박 후보는 박근혜·문재인·안철수 3자 가상(假想) 대결에선 늘 1위를 지켰으나 야당 단일 후보에겐 밀리는 결과가 몇 번이나 나타났다. 안 후보 사퇴 이후 박 후보에게 10%포인트 이상 뒤지던 문 후보의 지지율이 꾸준히 올라가 선거 이틀 전에는 뒤집히기도 했다.\\n\\n이 순간 박 당선인에게 가장 절실한 자세는 자신을 지지한 1600만 국민과 함께 자신의 경쟁자에게 표를 던진 1500만 국민의 마음을 정확히 읽고 그들을 진정으로 끌어안는 것이다. 이명박 정부 5년 동안 우리나라는 미국발(發) 금융위기·유럽발 재정위기 속에서도 세계 각국 가운데 거시(巨視)경제지표가 가장 빨리 호전되고 한 사회의 소득 불평등 정도를 가리키는 지니계수도 노무현 정부 동안 계속 악화하다 2009년 0.320에서 2011년 0.313으로 개선됐다. 그럼에도 국민은 이런 지표 개선과 관계없이 어쩌면 오히려 그것 때문에 더 큰 경제적 고통과 상대적 박탈감을 느껴왔다. 대기업의 경기가 좋아지면, 고소득자의 소득이 높아지면, 그 효과가 형편이 어려운 사람들에게도 퍼져간다는 이른바 '낙수(落水)효과'는 현실과 거리가 먼 것으로 드러났다.\\n\\nDB손해보험 다이렉트 바로가기\\n젊은이들은 대학을 졸업해도 번\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.body[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"19일 실시된 대통령선거에서 새누리당 박근혜 후보가 투표자 3072만명 중 51% 남짓 1600만표가량을 얻어 18대 대통령에 당선됐다. 민주통합당 문재인 후보는 48% 남짓 1500만표가량을 얻었다. 1987년 직선제 개헌 이후 득표율 50%를 넘긴 대통령은 이번이 처음이다. 70%를 약간 넘을 것으로 예상했던 투표율은 75.8%까지 올랐다. 대한민국 건국 이후 첫 여성 대통령이 탄생했다. 박 후보의 당선으로 투표율이 높으면 야당이 유리하다는 속설(俗說)도 깨졌다. 경쟁자 지지 국민의 박탈감 헤아려야  정권 재창출보다 정권 교체를 바라는 여론이 더 높은 가운데 치러진 이번 대선은 박 후보에게 버거운 선거였다. 선거 초반 한동안 유지되던 박근혜 대세론은 무소속 안철수 후보 등장으로 몇 차례 크게 흔들렸다. 박 후보는 박근혜·문재인·안철수 3자 가상(假想) 대결에선 늘 1위를 지켰으나 야당 단일 후보에겐 밀리는 결과가 몇 번이나 나타났다. 안 후보 사퇴 이후 박 후보에게 10%포인트 이상 뒤지던 문 후보의 지지율이 꾸준히 올라가 선거 이틀 전에는 뒤집히기도 했다.  이 순간 박 당선인에게 가장 절실한 자세는 자신을 지지한 1600만 국민과 함께 자신의 경쟁자에게 표를 던진 1500만 국민의 마음을 정확히 읽고 그들을 진정으로 끌어안는 것이다. 이명박 정부 5년 동안 우리나라는 미국발(發) 금융위기·유럽발 재정위기 속에서도 세계 각국 가운데 거시(巨視)경제지표가 가장 빨리 호전되고 한 사회의 소득 불평등 정도를 가리키는 지니계수도 노무현 정부 동안 계속 악화하다 2009년 0.320에서 2011년 0.313으로 개선됐다. 그럼에도 국민은 이런 지표 개선과 관계없이 어쩌면 오히려 그것 때문에 더 큰 경제적 고통과 상대적 박탈감을 느껴왔다. 대기업의 경기가 좋아지면, 고소득자의 소득이 높아지면, 그 효과가 형편이 어려운 사람들에게도 퍼져간다는 이른바 '낙수(落水)효과'는 현실과 거리가 먼 것으로 드러났다.  DB손해보험 다이렉트 바로가기 젊은이들은 대학을 졸업해도 번\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.body = data_raw.body.apply(lambda x : re.sub(\"\\n\", \" \", x) if re.findall(\"\\n\", x) else x)\n",
    "data_raw.body[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"19일 실시된 대통령선거에서 새누리당 박근혜 후보가 투표자 3072만명 중 51% 남짓 1600만표가량을 얻어 18대 대통령에 당선됐다. 민주통합당 문재인 후보는 48% 남짓 1500만표가량을 얻었다. 1987년 직선제 개헌 이후 득표율 50%를 넘긴 대통령은 이번이 처음이다. 70%를 약간 넘을 것으로 예상했던 투표율은 75.8%까지 올랐다. 대한민국 건국 이후 첫 여성 대통령이 탄생했다. 박 후보의 당선으로 투표율이 높으면 야당이 유리하다는 속설(俗說)도 깨졌다. 경쟁자 지지 국민의 박탈감 헤아려야  정권 재창출보다 정권 교체를 바라는 여론이 더 높은 가운데 치러진 이번 대선은 박 후보에게 버거운 선거였다. 선거 초반 한동안 유지되던 박근혜 대세론은 무소속 안철수 후보 등장으로 몇 차례 크게 흔들렸다. 박 후보는 박근혜·문재인·안철수 3자 가상(假想) 대결에선 늘 1위를 지켰으나 야당 단일 후보에겐 밀리는 결과가 몇 번이나 나타났다. 안 후보 사퇴 이후 박 후보에게 10%포인트 이상 뒤지던 문 후보의 지지율이 꾸준히 올라가 선거 이틀 전에는 뒤집히기도 했다.  이 순간 박 당선인에게 가장 절실한 자세는 자신을 지지한 1600만 국민과 함께 자신의 경쟁자에게 표를 던진 1500만 국민의 마음을 정확히 읽고 그들을 진정으로 끌어안는 것이다. 이명박 정부 5년 동안 우리나라는 미국발(發) 금융위기·유럽발 재정위기 속에서도 세계 각국 가운데 거시(巨視)경제지표가 가장 빨리 호전되고 한 사회의 소득 불평등 정도를 가리키는 지니계수도 노무현 정부 동안 계속 악화하다 2009년 0.320에서 2011년 0.313으로 개선됐다. 그럼에도 국민은 이런 지표 개선과 관계없이 어쩌면 오히려 그것 때문에 더 큰 경제적 고통과 상대적 박탈감을 느껴왔다. 대기업의 경기가 좋아지면, 고소득자의 소득이 높아지면, 그 효과가 형편이 어려운 사람들에게도 퍼져간다는 이른바 '낙수(落水)효과'는 현실과 거리가 먼 것으로 드러났다.  DB손해보험 다이렉트 바로가기 젊은이들은 대학을 졸업해도 번\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data_raw.body.values\n",
    "corpus = list(corpus)\n",
    "corpus[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.238 Gb\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "\n",
    "word_extractor = WordExtractor(\n",
    "    max_left_length=10,\n",
    "    max_right_length=6,\n",
    "    min_frequency=5\n",
    ")\n",
    "\n",
    "word_extractor.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 27258\n",
      "all branching entropies was computed # words = 42263\n",
      "all accessor variety was computed # words = 42263\n"
     ]
    }
   ],
   "source": [
    "scores = word_extractor.word_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scores(cohesion_forward=0.9183683615125757, cohesion_backward=0.24505110018405926, left_branching_entropy=2.3429356954455187, right_branching_entropy=0.90776970980902, left_accessor_variety=53, right_accessor_variety=13, leftside_frequency=1131, rightside_frequency=72)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['박근혜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_dataset(corpus):\n",
    "\n",
    "    # scores dict로 cohesion scores, ltokenizer 만들기\n",
    "    cohesion_scores = {word:score.cohesion_forward for word, score in scores.items()}\n",
    "    ltokenizer = LTokenizer(scores=cohesion_scores)\n",
    "\n",
    "    def l_tokenizer(word):\n",
    "        return ltokenizer.tokenize(word, remove_r=True)\n",
    "\n",
    "    # ltokenizer를 활용해서 벡터라이저 만들기\n",
    "    vectorizer = CountVectorizer(\n",
    "        tokenizer=l_tokenizer,\n",
    "    )\n",
    "\n",
    "    # x 만들기\n",
    "    x = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # vocab2int, int2vocab 만들기\n",
    "    vocab2int = vectorizer.vocabulary_\n",
    "    int2vocab = [\n",
    "        word for word, index in sorted(vocab2int.items(), key=lambda x:x[1])\n",
    "    ]\n",
    "    \n",
    "    return x, vocab2int, int2vocab\n",
    "\n",
    "x, vocab2int, int2vocab = make_dataset(corpus)\n",
    "\n",
    "def word2int(word):\n",
    "    return vocab2int.get(word, -1)\n",
    "\n",
    "def int2word(idx):\n",
    "    if 0 <= idx < len(int2vocab):\n",
    "        return int2vocab[idx]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_sparse.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,022 중 965 문서가 \"박근혜\" 라는 단어를 가지고 있습니다.\n",
      "1,022 중 907 문서가 \"대통령\" 라는 단어를 가지고 있습니다.\n",
      "1,022 중  93 문서가 \"당선인\" 라는 단어를 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "keyword_list = [\"박근혜\", \"대통령\", \"당선인\"]\n",
    "\n",
    "for word in keyword_list:\n",
    "\n",
    "    word_idx = word2int(word)\n",
    "    positive_document = x[:, word_idx].nonzero()[0]\n",
    "\n",
    "    print('{:3,} 중 {:3,} 문서가 \"{}\" 라는 단어를 가지고 있습니다.'.format(x.shape[0], len(positive_document), word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,022 중 965 문서가 \"박근혜\" 라는 단어를 가지고 있습니다.\n",
      "Counter({1: 965, -1: 57})\n"
     ]
    }
   ],
   "source": [
    "word = \"박근혜\"\n",
    "word_idx = word2int(word)\n",
    "positive_document = x[:, word_idx].nonzero()[0]\n",
    "\n",
    "print('{:3,} 중 {:3,} 문서가 \"{}\" 라는 단어를 가지고 있습니다.'.format(x.shape[0], len(positive_document), word))\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "y = [1 if i in positive_document else -1 for i in range(x.shape[0])]\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(word):\n",
    "    \n",
    "    word_idx = word2int(word) # word2int 함수 필요\n",
    "    positive_document = x[:, word_idx].nonzero()[0] # x 데이터셋 필요\n",
    "    \n",
    "    def get_label(i):\n",
    "        return 1 if i in positive_document else -1\n",
    "    \n",
    "    y_train = [get_label(i) for i in range(x.shape[0])]\n",
    "    \n",
    "    (row, col) = x.nonzero()\n",
    "    data = x.data\n",
    "    \n",
    "    row_ = []\n",
    "    col_ = []\n",
    "    data_ = []\n",
    "    \n",
    "    for r, c, d in zip(row, col, data):\n",
    "        if c == word_idx:\n",
    "            continue\n",
    "        row_.append(r)\n",
    "        col_.append(c)\n",
    "        data_.append(d)\n",
    "        \n",
    "    from scipy.sparse import csr_matrix\n",
    "    x_train = csr_matrix((data_, (row_, col_)))\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "x_train, y_train = get_train_data('박근혜')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-eff592d08a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogistic_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogistic_l1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlasso_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    870\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m    871\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_l1 = LogisticRegression(penalty='l1', C=10)\n",
    "logistic_l1.fit(x_train, y_train)\n",
    "\n",
    "def lasso_keyword(word, C=20, topk=20):\n",
    "    if not (word in vocab2int):\n",
    "        return []\n",
    "    \n",
    "    x_train, y_train = get_train_data(word)\n",
    "    logistic_l1 = LogisticRegression(penalty='l1', C=C)\n",
    "    logistic_l1.fit(x_train, y_train)\n",
    "    \n",
    "    idx_coef = enumerate(logistic_l1.coef_.reshape(-1))\n",
    "    sorted_coefficients = sorted(idx_coef, key=lambda x:-x[1])\n",
    "    \n",
    "    # filtering keyword\n",
    "    keywords = [word_idx for word_idx, coef in sorted_coefficients[:topk] \n",
    "                if coef > 0.001]\n",
    "    \n",
    "    # decode idx to str\n",
    "    keywords = [int2word(word_idx) for word_idx in keywords]\n",
    "    return keywords\n",
    "\n",
    "print(lasso_keyword('박근혜'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "있어\n",
      "당선인\n",
      "계엄\n",
      "한·미\n",
      "촛불\n",
      "요구\n",
      "도발\n",
      "관계\n",
      "허위\n",
      "한다.\n"
     ]
    }
   ],
   "source": [
    "# idx_coef = enumerate(logistic_l1.coef_.reshape(-1))\n",
    "# sorted_coefficients = sorted(idx_coef, key=lambda x:-x[1])\n",
    "\n",
    "# for word_idx, coef in sorted_coefficients[:10]:\n",
    "#     if coef < 0.001: break\n",
    "#     print(int2word(word_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 노트\n",
    "\n",
    "1. customized_tokenizer가 명사, 용언만 가져오지 않기 때문에 \"있어\", \"한다.\"처럼 불필요한 단어가 상위에 등장한다.\n",
    "> 사설 분석에 적합한 토크나이저가 필요하다.\n",
    "\n",
    "2. 시기 별 키워드 분석을 위해서는 월 단위로 기사 분류.\n",
    "3. 월(혹은 분기) 단위로 워드클라우드 구현.\n",
    "4. 전체 사설 데이터를 활용하는게 더 나을 수 있다.\n",
    "5. 옛날 신문 같은 경우에는 한자를 한글로 변환해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012.12</td>\n",
       "      <td>[사설] 박근혜 당선인, 겸허하게 온 국민 껴안는 걸로 시작하라</td>\n",
       "      <td>19일 실시된 대통령선거에서 새누리당 박근혜 후보가 투표자 3072만명 중 51% ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.12</td>\n",
       "      <td>[사설] 선거 여론조사, '公表 금지 기간' 없애거나 더 단축을</td>\n",
       "      <td>국민은 여론조사 공표가 금지된 지난 13일부터 18일까지 6일 동안 여론 흐름을 전...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012.12</td>\n",
       "      <td>[사설] 한국 진보 左派, 進化하지 않으면 몰락한다</td>\n",
       "      <td>민주당 문재인 후보는 범(汎)야권 후보를 완벽하게 단일화하고 투표율이 자신들이 야당...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012.12</td>\n",
       "      <td>[사설] 政權 인수, 소리 내지 말고 실무적으로 하라</td>\n",
       "      <td>곧 박근혜대통령직인수위원회가 발족한다. 역대 대통령직인수위에 대한 국민 기억은 그다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012.12</td>\n",
       "      <td>[사설] '5060世代 지혜' 못지않게 '2030 氣 살리기'도 중요하다</td>\n",
       "      <td>19일의 대선에서 50대(代) 투표율이 89.9%였던 것으로 방송사 출구 조사에서 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date                                     title  \\\n",
       "0  2012.12       [사설] 박근혜 당선인, 겸허하게 온 국민 껴안는 걸로 시작하라   \n",
       "1  2012.12       [사설] 선거 여론조사, '公表 금지 기간' 없애거나 더 단축을   \n",
       "2  2012.12              [사설] 한국 진보 左派, 進化하지 않으면 몰락한다   \n",
       "3  2012.12             [사설] 政權 인수, 소리 내지 말고 실무적으로 하라   \n",
       "4  2012.12  [사설] '5060世代 지혜' 못지않게 '2030 氣 살리기'도 중요하다   \n",
       "\n",
       "                                                body  \n",
       "0  19일 실시된 대통령선거에서 새누리당 박근혜 후보가 투표자 3072만명 중 51% ...  \n",
       "1  국민은 여론조사 공표가 금지된 지난 13일부터 18일까지 6일 동안 여론 흐름을 전...  \n",
       "2  민주당 문재인 후보는 범(汎)야권 후보를 완벽하게 단일화하고 투표율이 자신들이 야당...  \n",
       "3  곧 박근혜대통령직인수위원회가 발족한다. 역대 대통령직인수위에 대한 국민 기억은 그다...  \n",
       "4  19일의 대선에서 50대(代) 투표율이 89.9%였던 것으로 방송사 출구 조사에서 ...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_revise = data_raw.copy()\n",
    "data_revise.date = data_revise.date.apply(lambda x : x[:7])\n",
    "data_revise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기간 별 텍스트 데이터 취합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2012.12', '2013.01', '2013.02', '2013.03', '2013.04', '2013.05',\n",
       "       '2013.06', '2013.07', '2013.08', '2013.09', '2013.10', '2013.11',\n",
       "       '2013.12', '2014.01', '2014.02', '2014.03', '2014.04', '2014.05',\n",
       "       '2014.06', '2014.07', '2014.08', '2014.09', '2014.10', '2014.11',\n",
       "       '2014.12', '2015.01', '2015.02', '2015.03', '2015.04', '2015.05',\n",
       "       '2015.06', '2015.07', '2015.08', '2015.09', '2015.10', '2015.11',\n",
       "       '2015.12', '2016.01', '2016.02', '2016.03', '2016.04', '2016.05',\n",
       "       '2016.06', '2016.07', '2016.08', '2016.09', '2016.10', '2016.11',\n",
       "       '2016.12', '2017.01', '2017.02', '2017.03', ': 2016.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month = np.unique(data_revise.date)\n",
    "month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, title, body]\n",
       "Index: []"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = pd.DataFrame(columns=['date', 'title', 'body'])\n",
    "\n",
    "for dates in month:\n",
    "\n",
    "    # data revise에서 샘플링\n",
    "    sample = data_revise[data_revise.date == dates]\n",
    "\n",
    "    titles = sample.title.values\n",
    "    titles = list(titles)\n",
    "    titles = ''.join(titles)\n",
    "    \n",
    "    articles = sample.body.values\n",
    "    articles = list(articles)\n",
    "    articles = ''.join(articles)\n",
    "\n",
    "    data = {\n",
    "        \"date\" : dates,\n",
    "        \"title\" : titles,\n",
    "        \"body\" : articles\n",
    "    }\n",
    "    \n",
    "    data_merged.loc[len(data_merged)] = data\n",
    "    \n",
    "data_merged.to_pickle('cs_data_merged_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 \n",
      " <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "corpus = data_merged.body.values\n",
    "corpus = list(corpus)\n",
    "\n",
    "print(len(corpus), \"\\n\", type(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_dataset(corpus):\n",
    "\n",
    "    # scores dict로 cohesion scores, ltokenizer 만들기\n",
    "    cohesion_scores = {word:score.cohesion_forward for word, score in scores.items()}\n",
    "    ltokenizer = LTokenizer(scores=cohesion_scores)\n",
    "\n",
    "    def l_tokenizer(word):\n",
    "        return ltokenizer.tokenize(word, remove_r=True)\n",
    "\n",
    "    # ltokenizer를 활용해서 벡터라이저 만들기\n",
    "    vectorizer = CountVectorizer(\n",
    "        tokenizer=l_tokenizer,\n",
    "    )\n",
    "\n",
    "    # x 만들기\n",
    "    x = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    # vocab2int, int2vocab 만들기\n",
    "    vocab2int = vectorizer.vocabulary_\n",
    "    int2vocab = [\n",
    "        word for word, index in sorted(vocab2int.items(), key=lambda x:x[1])\n",
    "    ]\n",
    "    \n",
    "    return x, vocab2int, int2vocab\n",
    "\n",
    "x, vocab2int, int2vocab = make_dataset(corpus)\n",
    "\n",
    "def word2int(word):\n",
    "    return vocab2int.get(word, -1)\n",
    "\n",
    "def int2word(idx):\n",
    "    if 0 <= idx < len(int2vocab):\n",
    "        return int2vocab[idx]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x train, y train 으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(word):\n",
    "    \n",
    "    word_idx = word2int(word) # word2int 함수 필요\n",
    "    positive_document = x[:, word_idx].nonzero()[0] # x 데이터셋 필요\n",
    "    \n",
    "    def get_label(i):\n",
    "        return 1 if i in positive_document else -1\n",
    "    \n",
    "    y_train = [get_label(i) for i in range(x.shape[0])]\n",
    "    \n",
    "    (row, col) = x.nonzero()\n",
    "    data = x.data\n",
    "    \n",
    "    row_ = []\n",
    "    col_ = []\n",
    "    data_ = []\n",
    "    \n",
    "    for r, c, d in zip(row, col, data):\n",
    "        if c == word_idx:\n",
    "            continue\n",
    "        row_.append(r)\n",
    "        col_.append(c)\n",
    "        data_.append(d)\n",
    "        \n",
    "    from scipy.sparse import csr_matrix\n",
    "    x_train = csr_matrix((data_, (row_, col_)))\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "x_train, y_train = get_train_data('박근혜')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-da323979af53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogistic_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogistic_l1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    870\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m    871\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_l1 = LogisticRegression(penalty='l1', C=10)\n",
    "logistic_l1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-277-eff592d08a92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlogistic_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogistic_l1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlasso_keyword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1302\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    870\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m    871\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_l1 = LogisticRegression(penalty='l1', C=10)\n",
    "logistic_l1.fit(x_train, y_train)\n",
    "\n",
    "def lasso_keyword(word, C=20, topk=20):\n",
    "    if not (word in vocab2int):\n",
    "        return []\n",
    "    \n",
    "    x_train, y_train = get_train_data(word)\n",
    "    logistic_l1 = LogisticRegression(penalty='l1', C=C)\n",
    "    logistic_l1.fit(x_train, y_train)\n",
    "    \n",
    "    idx_coef = enumerate(logistic_l1.coef_.reshape(-1))\n",
    "    sorted_coefficients = sorted(idx_coef, key=lambda x:-x[1])\n",
    "    \n",
    "    # filtering keyword\n",
    "    keywords = [word_idx for word_idx, coef in sorted_coefficients[:topk] \n",
    "                if coef > 0.001]\n",
    "    \n",
    "    # decode idx to str\n",
    "    keywords = [int2word(word_idx) for word_idx in keywords]\n",
    "    return keywords\n",
    "\n",
    "print(lasso_keyword('박근혜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 노트\n",
    "\n",
    "- 유사어는 특정 키워드를 동일하게 가지고 있는 문서 안에 있는 키워드를 유사어로 지정한다. \n",
    "> 그러므로 차라리 월 별 코퍼스의 키워드를 분석하는 게 더 나을 것 같다.\n",
    "\n",
    "- 대부분 \"박근혜\" 키워드를 가지고 있으므로, 연관어 / 키워드를 분류하는게 애초에 불가능하다. Lasso를 사용하는 경우는, logistic regression은 대표벡터를 학습해야 하는데 위 코퍼스는 전부 다 \"박근혜\"라는 단어를 포함하고 있어, 2개 이상 집단으로 나뉘지 않아 2개 이상의 대표벡터가 존재하지 않기 때문이다.\n",
    "> 그렇다면 \"박근혜\"를 코퍼스에서 제거한 후 연관어, 키워드를 분석하면 어떨까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드 추출(선회)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3101 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3101 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixbasedKeywordExtractor trained\n"
     ]
    }
   ],
   "source": [
    "from soykeyword.proportion import MatrixbasedKeywordExtractor\n",
    "\n",
    "proportion_based_extractor = MatrixbasedKeywordExtractor(\n",
    "    min_tf=0, \n",
    "    min_df=0,\n",
    "    verbose=True)\n",
    "\n",
    "proportion_based_extractor.train(x, int2vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-3dc3b3e99f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproportion_based_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_from_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'박근혜'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/soykeyword/proportion/_proportion.py\u001b[0m in \u001b[0;36mextract_from_word\u001b[0;34m(self, word, min_frequency, min_score)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_from_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_document_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/soykeyword/proportion/_proportion.py\u001b[0m in \u001b[0;36mextract_from_docs\u001b[0;34m(self, docs, min_frequency, min_score)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_negative_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sum_to_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mnp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sum_to_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/soykeyword/proportion/_proportion.py\u001b[0m in \u001b[0;36m_sum_to_proportion\u001b[0;34m(self, sum_dict)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sum_to_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0msum_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msum_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_positive_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/soykeyword/proportion/_proportion.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sum_to_proportion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0msum_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msum_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_positive_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "keywords = proportion_based_extractor.extract_from_word('박근혜')\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 노트\n",
    "\n",
    "- \"박근혜\"가 포함된 문서의 키워드를 추출하려니, 확률 기반 추출방법도 제대로 되지 않는다. \n",
    "> 우선 가장 간단한 방법인 워드 클라우드부터 시도해봐야겠다. 월 별 가장 높은 빈도가 나오는 명사, 용언만 추출해서 보여주도록."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
